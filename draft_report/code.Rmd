---
title: "Advandced Data Science Project RMD"
author: "Nikita Stempniewicz"
date: "September 8, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Note: I am not done yet but wanted to take the opportunity to have you look over the general idea and contents. Before the final draft I will work on cleaning up the text and figures, and finishing the last section and conclusion. 


```{r, eval=FALSE, include=FALSE}
#this is the code I used to scrape glassdoor

##Used some of stephens code from lab for help, need to figure out how to cite?
###################################################################################################
###################################################################################################
##Load Packages
###################################################################################################
###################################################################################################
library(rvest)
library(stringr)

###################################################################################################
##  Urls for data scientist positions on glassdoor 
##  note for some reason glassdoor does not let you go past job post page 33?
###################################################################################################

url <- paste0("https://www.glassdoor.com/Job/new-york-data-scientist-jobs-SRCH_IL.0,8_IC1132348_KO9,23_IP",1,".htm")
urls <- paste0("https://www.glassdoor.com/Job/new-york-data-scientist-jobs-SRCH_IL.0,8_IC1132348_KO9,23_IP", 1:33, ".htm")

df1<-data.frame()

####################################################################################################
####################################################################################################
####################################################################################################
## First we will get data from summary data for each post from the search result urls
####################################################################################################
####################################################################################################
####################################################################################################


###################################################################################################
##  creates fields which are the individual job posts from glassdoor
###################################################################################################
for (i in 1:33){
  paste(i)
  download.file(urls[i], destfile = "scrapedpage.html", quiet=TRUE)
  fields <-read_html("scrapedpage.html") %>% html_nodes(xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "jl", " " ))]')
  
  ###################################################################################################
  ##  creates job title which are the individual job posts 
  ###################################################################################################
  title <- fields %>% html_nodes(".flexbox .jobLink") %>% html_text() %>% trimws()
  
  ###################################################################################################
  ## gets salaries and replaces not listed when salary information is missing
  ###################################################################################################
  salaries <- sapply(fields, function(x) {
    tmp <- html_nodes(x, ".small") %>% html_text()
    ifelse(length(tmp) == 0, "Not listed", trimws(tmp))
  })
  salaries <- trimws(gsub("(Glassdoor est.)", "", salaries))
  
  ####################################################################################################
  ##  gets job location 
  ###################################################################################################
  
  locations <- fields %>% html_nodes(".loc") %>% html_text() %>% trimws()
  
  
  ####################################################################################################
  ##  gets emplyer, employer id and job id 
  ###################################################################################################
  
  ##can;t figure out how to get gsub or strreplace to recognize the - to remove the city name
  employer <- fields %>% html_nodes(".empLoc") %>% html_text() %>% trimws()
  
  employer_id<-fields %>% html_attr("data-emp-id")
  
  job_id<-fields %>% html_attr("data-id")
  
  
  ####################################################################################################
  ##  create and append data frame with job posts
  ###################################################################################################
  df1a<-cbind(job_id, employer_id, title, salaries, locations, employer)
  
  df1<-rbind(df1,df1a)
}
 
save(df1, file="glassdoor_df1.r")

####################################################################################################
####################################################################################################
####################################################################################################
## Now we will get data from accessing the websites for the individual job posts using
####################################################################################################
####################################################################################################
####################################################################################################


####################################################################################################
##  creates job urls from job id, note if this part breaks, just replace the first part of the past 0 with another link
###################################################################################################

 df1$job.urls <- paste0("https://www.glassdoor.com/job-listing/data-scientist-emc-research-JV_IC1145845_KO0,14_KE15,27.htm?jl=",
                     df1$job_id)
  
####################################################################################################
## gets the raw job description, days posted, and employed from the job posts
###################################################################################################
  
  df1$job_desc_raw<-""
  df1$employer2<-""
#used to compare null results in loop  
  a <- character(0)
  n<-nrow(df1)
  for (i in 1:n)
    {

    Sys.sleep(1)
    
      download.file(df1[i,]$job.urls, destfile = "scrapedpage.html", quiet=TRUE)
        
        website<- read_html("scrapedpage.html")

        df1[i,]$job_desc_raw <- 
            (if (identical(a,(website %>% html_nodes("#JobDescContainer") %>% html_text())))
              {
               "NO DESCRIPTION LISTED"
              } 
            
            else
              {
                website %>% html_nodes("#JobDescContainer") %>% html_text()
              }
            )        
    
        df1[i,]$employer2 <- 
          (if (identical(a,(website  %>% html_nodes(".padRtSm") %>% html_text())))
            {
      
              "NO EMPLOYER LISTED"
            } 
           
           else
            {
              website  %>% html_nodes(".padRtSm") %>% html_text()
            }
           )
  }
  
  saveRDS(df1, file="glassdoor_df1")

####################################################################################################
####################################################################################################
####################################################################################################
## Finally, we will get data from acccessing the employers glassdoor page
####################################################################################################
####################################################################################################
####################################################################################################

####################################################################################################
##  creates employer urls from job id, note if this part breaks, just replace the first part of the past 0 with another link
###################################################################################################
  
  unique_emp_ids<-unlist(unique(df1$employer_id[df1$employer_id!=0]))
  
  emp.urls <- paste0("https://www.glassdoor.com/Overview/Working-at-ID-Analytics-EI_IE",
                     unique_emp_ids,".11,23.htm")
  
  df2<-data.frame(unique_emp_ids, emp.urls)
  
####################################################################################################
##  gets empoyer description text
###################################################################################################

  n<-nrow(df2)

  df2$emp_desc_raw<-""

 # df2<-df2[-c(387),]  
      
  for (i in 1:n)
  {
    
  Sys.sleep(1)
    
    download.file(as.character(df2$emp.urls[i]), destfile = "scrapedpage.html", quiet=TRUE)
    
    website<- read_html("scrapedpage.html")
    
    df2[i,]$emp_desc_raw <- 
      (if (identical(a,(website %>% html_nodes("#EmpBasicInfo") %>% html_text())))
      {
        "NO DESCRIPTION LISTED"
      } 
      
      else
      {
        website %>% html_nodes("#EmpBasicInfo") %>% html_text()
      }
      )        
    
  }
  
  saveRDS(df2, file="glassdoor_df2")
  
  
####################################################################################################
####################################################################################################
####################################################################################################
####################################################################################################
##  merge job and employer datasets
####################################################################################################
#################################################################################################### 
#################################################################################################### 
####################################################################################################

colnames(df2)[1] <- "employer_id"
  
glassdoor_df<-merge(x = df1, y = df2, by = "employer_id", all.x = TRUE)

saveRDS(glassdoor_df, file="glassdoor_df")




```

```{r, eval=FALSE, include=FALSE}

## This is the code i used to extract data from the raw job and employer description


##################################################################################################################
##################################################################################################################
# Load glass door dataset
##################################################################################################################
##################################################################################################################

library(tidytext)
library(vcd)
library(xtable)


data<-readRDS("glassdoor_df")

n<-nrow(data)

##################################################################################################################
##################################################################################################################
# Create structured fields from raw job descriptions
##################################################################################################################
##################################################################################################################


bad_txt<-c("â", "-", ",", "/")
data$job_desc1<-gsub(paste(bad_txt, collapse="|"), " ", data$job_desc_raw)
data$job_desc2<-gsub("([[:lower:]])([[:upper:]])", "\\1 \\2", data$job_desc1)
data$job_desc2<-gsub("([[:upper:]][[:upper:]][[:upper:]])([[:upper:]][[:lower:]])", "\\1 \\2", data$job_desc2)
data$job_desc3<-gsub("[(]", " ", data$job_desc2)
data$job_desc3<-gsub("[)]", " ", data$job_desc3)


bachelor<-c("\\bbachelor\\b", "\\bbachelors\\b", "\\bundergraduate\\b", "\\bbs\\b", "\\bb.s.\\b", "\\bb.s\\b")
master<-c("\\bmaster\\b", "\\bmasters\\b", "graduate degree", "ms degree", "ms in", "m.s. in", "m.s in"  )
phd<-c("phd", "doctorate", "\\bph\\b")
mba<-c("m.b.a", "\\mba\\b")
stats<-c("statistics", "statistical", "regression", "modelling")




for (i in 1:n)
{
  
data$python[i] <- any(grepl("\\bpython\\b", data$job_desc3[i], ignore.case=TRUE))
data$ml[i] <- any(grepl("machine learning", data$job_desc3[i], ignore.case=TRUE))
data$opt[i] <- any(grepl("optimization", data$job_desc3[i], ignore.case=TRUE))
data$stats[i] <- any(grepl("statistic", data$job_desc3[i], ignore.case=TRUE))
data$risk[i] <- any(grepl("risk", data$job_desc3[i], ignore.case=TRUE))
data$UX[i] <- any(grepl("UX", data$job_desc3[i], ignore.case=FALSE))
data$bd[i] <- any(grepl("big data", data$job_desc3[i], ignore.case=TRUE))
data$dm[i] <- any(grepl("data management", data$job_desc3[i], ignore.case=TRUE))
data$pharma[i] <- any(grepl("pharmaceutical", data$job_desc3[i], ignore.case=TRUE))
data$fs[i] <- any(grepl("financial services", data$job_desc3[i], ignore.case=TRUE))
data$sd[i] <- any(grepl("software development", data$job_desc3[i], ignore.case=TRUE))
data$program[i] <- any(grepl("programming", data$job_desc3[i], ignore.case=TRUE))
data$research[i] <- any(grepl("research", data$job_desc3[i], ignore.case=TRUE))
data$R[i] <- any(grepl("\\bR\\b", data$job_desc3[i], ignore.case=TRUE))
data$SAS[i] <- any(grepl("\\bSAS\\b", data$job_desc3[i], ignore.case=TRUE))
data$C[i] <- any(grepl("\\bC+\\b", data$job_desc3[i], ignore.case=TRUE))
data$stata[i] <- any(grepl("\\bstata\\b", data$job_desc3[i], ignore.case=TRUE))
data$SQL[i] <- any(grepl("\\bsql\\b", data$job_desc3[i], ignore.case=TRUE))
data$excel[i] <- any(grepl("\\bexcel\\b", data$job_desc3[i], ignore.case=TRUE))
data$tableau[i] <- any(grepl("\\btableau\\b", data$job_desc3[i], ignore.case=TRUE))
data$spss[i] <- any(grepl("\\bspss\\b", data$job_desc3[i], ignore.case=TRUE))
data$java[i] <- any(grepl("\\bjava\\b", data$job_desc3[i], ignore.case=TRUE))
data$linux[i] <- any(grepl("\\blinux\\b", data$job_desc3[i], ignore.case=TRUE))
data$matlab[i] <- any(grepl("\\bmatlab\\b", data$job_desc3[i], ignore.case=TRUE))
data$NLP[i] <- any(grepl("\\bNLP\\b", data$job_desc3[i], ignore.case=TRUE))
data$hadoop[i] <- any(grepl("\\bhadoop\\b", data$job_desc3[i], ignore.case=TRUE))
data$ruby[i] <- any(grepl("\\bruby\\b", data$job_desc3[i], ignore.case=TRUE))
data$oracle[i] <- any(grepl("\\boracle\\b", data$job_desc3[i], ignore.case=TRUE))
data$sas[i] <- any(grepl("\\bsas\\b", data$job_desc3[i], ignore.case=TRUE))
data$bs[i] <- any(grepl(paste(bachelor, collapse="|"), data$job_desc3[i], ignore.case=TRUE))
data$bs2[i] <- any(grepl("bachelor", data$job_desc3[i], ignore.case=TRUE))
data$masters[i] <- any(grepl(paste(master, collapse="|"), data$job_desc3[i], ignore.case=TRUE))
data$phd[i] <- any(grepl(paste(phd, collapse="|"), data$job_desc3[i], ignore.case=TRUE))
data$mba[i] <- any(grepl(paste(mba, collapse="|"), data$job_desc3[i], ignore.case=TRUE))
data$stats[i] <- any(grepl(paste(stats, collapse="|"), data$job_desc3[i], ignore.case=TRUE))
data$degree[i] <- any(grepl("\\bdegree\\b", data$job_desc3[i], ignore.case=TRUE))

}

#data$skills<- data$degree+data$phd+data$masters+data$phd+data$bs2+data$bs+data$sas+data$oracle+data$ruby+data$hadoop+data$NLP+data$matlab+data$linux+
#  data$java+ data$spss +data$spss +data$tableau+ data$excel+data$SQL+data$stata + data$C + data$SAS +data$R+ data$python

data$skills2<- data$sas+data$oracle+data$ruby+data$hadoop+data$NLP+data$matlab+data$linux+
  data$java+ data$spss +data$spss +data$tableau+ data$excel+data$SQL+data$stata + data$C + data$SAS +data$R+ 
  data$python+data$ml+data$opt+data$bd+data$research+data$stats+data$dm+data$risk+data$fs+data$program+data$pharma+data$sd++data$UX

data$education<- data$masters+data$phd+data$bs


#table(data$skills2)
#table(data$education)

#saveRDS(data, file="glassdoor_df_cleaned")


#data$job_id[data$skills2==0]
#strsplit(data$job_desc3[data$job_id=="2426165858"]," ")

##################################################################################################################
##################################################################################################################
# Create structured fields from raw employer descriptions
##################################################################################################################
##################################################################################################################


data$emp_desc1<-gsub(paste(bad_txt, collapse="|"), " ", data$emp_desc_raw)
data$emp_desc2<-gsub("([[:lower:]])([[:upper:]])", "\\1 \\2", data$emp_desc1)
data$emp_desc2<-gsub("([[:lower:]])([[:digit:]])", "\\1 \\2", data$emp_desc2)
data$emp_desc2<-gsub("([[:digit:]])([[:upper:]])", "\\1 \\2", data$emp_desc2)

 sub(".*years experience *(.*?)", "\\1", data$job_desc3[1], ignore.case=TRUE)

 data$job_desc3[1]

for (i in 1:n)
{
  
  data$founded[i] <- any(grepl("\\bfounded\\b", data$emp_desc2[i], ignore.case=TRUE))
  data$industry[i] <- sub(".*Industry *(.*?) *Revenue.*", "\\1", data$emp_desc2[i], ignore.case=TRUE)
  data$revenue[i] <- any(grepl("\\brevenue\\b", data$emp_desc2[i], ignore.case=TRUE))
  
  
}



for (i in 1:n)
{
  
  data$salary_low[i] <- sub(".*[$] *(.*?) *k[-].*", "\\1", data$salaries[i], ignore.case=TRUE)
  data$salary_high[i] <- sub(".*[$] *(.*?) *k[-].*", "\\1", data$salaries[i], ignore.case=TRUE)
  
}

#strsplit(data$job_desc3[data$job_id=="2525091352"]," ")

data$salary_low[data$salary_low=="Not listed"]<-""
data$salary_high[data$salary_high=="Not listed"]<-""


data$salary_average<-as.numeric(data$salary_low)+as.numeric(data$salary_high)

hist(data$salary_average, breaks=seq(0, 400, 25))


#summary(lm(data$salary_average~data$phd+data$masters+data$mba+data$bs))

#model1<-(lm(data$salary_average~ data$sas+data$oracle+data$ruby+data$hadoop+data$NLP+data$matlab+data$linux+
#  data$java+ data$spss +data$spss +data$tableau+ data$excel+data$SQL+data$stata + data$C + data$SAS +data$R+ 
#  data$python+data$ml+data$opt+data$bd+data$research+data$stats+data$dm+data$risk+data$fs+data$program+data$pharma+data$sd++data$UX))


#hist(data$salary_average, breaks=seq(0, 400, 25), xlab="Glassdoor Estimated Salary", main="Distribution of Estimated Salary for \nData Scientist Positions in the New York City Area")

#summary(data$salary_average)

saveRDS(data, file="glassdoor_df_cleaned")

#data<-readRDS("glassdoor_df_cleaned")
#data<-data[data$job_desc_raw!="NO DESCRIPTION LISTED",]
#data<-data[(is.na(data$salary_average)==FALSE),]

#table(data$education)

#myvars <- c("phd", "masters", "bs")
#educ_data <- data[myvars]

#data$phd2<- ifelse(educ_data$phd == FALSE, "", "PhD")
#data$ms2<- ifelse(educ_data$masters == FALSE, "", "MS")
#data$bs2<- ifelse(educ_data$bs == FALSE, "", "BS")

#data$educ_cat<-str_trim(paste(data$bs2, data$ms2, data$phd2, sep=" "))

#add bs and phd to bs ms and phd
#data$educ_cat<-ifelse(data$educ_cat == "BS  PhD", "BS MS PhD" , data$educ_cat)

#add ms and phd to phd
#data$educ_cat<-ifelse(data$educ_cat == "MS PhD", "PhD" , data$educ_cat)
#educ_cleaned$educ_cat<-ifelse(educ_cleaned$educ_cat == "MS PhD", "PhD" , educ_cleaned$educ_cat)

#add bs and ms to ms
#data$educ_cat<-ifelse(data$educ_cat == "BS MS", "MS" , data$educ_cat)
#educ_cleaned$educ_cat<-ifelse(educ_cleaned$educ_cat == "BS MS", "MS" , educ_cleaned$educ_cat)


```





```{r, include=FALSE}

library(ggplot2)
library(crayon)
library(huxtable)
library(tidytext)
library(vcd)
library(xtable)
library(stringr)
library(magrittr)

```

## Introduction

In general, a data scientist primary function is extracting insights from data, and communicating those insights in a clear and efficient manner. This requires a diverse skill set based in math, statistics, and computer, and information sciences. There is no typical technical or educational skill set required of data scientist, and different employers will require varrying skills. Some companies only require a bachelors degree while others a doctoral for data scientist. Most require programming and statistics skills, but often vary in the programming language or statistical software, e.g., SQL, R, STATA, Python, etc. Some data scientist position are more focused on data vizualization and communication, with an emphasis on skills with data vizualization software such as tableau, and others on exploratory analysis whith more of an emphasis on statistical methods such as machine learning.

### Specific Aims

  * Describe educational, technical, and professional requirements for data scientist positions
  * Describe estimated salaries for data scientist
  * Investigate the relationship between the different requirements and estimated salary

### Data Source

All the data for this analysis was scraped from Glassdoor.com. To minimize the variation in salaries due to geographic differences, the location when searching for data scientist positions was restricted to New York City, which includes the city itself and surrounding areas.

The methods used to scrape data is described below:

  * First, I get data, i.e., job  id, employer id, job title, salaries (if available), locations, employer from the search results for data scientist.
  * Next, using the job id from the search results in the previous step, I build the URL for the individual job posts on glassdoor and get the raw job descriptions which includes text information on qualifications which I will later query to build structured fields.
  * Finally, using the employer id from the search results, I build the URL for the employer page on glassdoor, and get the raw text from the employer description field, which includes things like, size, industry, year founded.

### Data set

  * 990 data scientist positions in the New York City area
      + No job descriptions (n=1)
      + No estimated salary (n=242)
  * 747 data scientist positions in the New York City area with a job description and estimated salary

###Estimated Salaries

```{r, echo=FALSE}
data<-readRDS("glassdoor_df_cleaned")
data<-data[data$job_desc_raw!="NO DESCRIPTION LISTED",]
data<-data[(is.na(data$salary_average)==FALSE),]

hist(data$salary_average, breaks=seq(0, 400, 25), xlab="Glassdoor Estimated Salary (in thousands)", main="Distribution of Estimated Salary for \nData Scientist Positions in the New York City Area")

mean<-round(mean(data$salary_average),1)
sd<-round(sd(data$salary_average),1)

```

The average estimated salary for data scientist in the new york city area was `r mean` and the standard deviation `r sd`

### Education

It is not uncommon for employers to have required and preffered qualifications which often include multiple possibilities for educational requirements, e.g., requiring a bachelors at a minumum, but specyfying a preference for candidates with a masters or PhD. When more than 1 educational requirements are mentioned we classify jobs to the highest level, e.g., posts that included BS and MS are considered MS.

```{r echo=FALSE}

#myvars <- c("phd", "masters", "bs")
#educ_data <- data[myvars]

data$phd2<- ifelse(data$phd == FALSE, "", "PhD")
data$ms2<- ifelse(data$masters == FALSE, "", "MS")
data$bs2<- ifelse(data$bs == FALSE, "", "BS")

data$educ_cat<-str_trim(paste(data$bs2, data$ms2, data$phd2, sep=" "))

#add bs and phd to bs ms and phd
data$educ_cat<-ifelse(data$educ_cat == "BS  PhD", "PhD" , data$educ_cat)

#add ms and phd to phd
data$educ_cat<-ifelse(data$educ_cat == "MS PhD", "PhD" , data$educ_cat)

#add bs ms phd to phd
data$educ_cat<-ifelse(data$educ_cat == "BS MS PhD", "PhD" , data$educ_cat)

#add bs and ms to ms
data$educ_cat<-ifelse(data$educ_cat == "BS MS", "MS" , data$educ_cat)

no_educ<-round((100*mean(data$educ_cat=="")),1)
all_educ<-round((100*mean(data$educ_cat=="BS MS PhD")),1)
phd<-round(100*mean(data$educ_cat=="PhD"),1)
masters<-round(100*mean(data$educ_cat=="MS"),1)
bs<-round(100*mean(data$educ_cat=="BS"),1)

educ_sum<-c(phd, masters, bs, all_educ, no_educ)
label<-c("phd", "masters", "bs", "all_educ", "no_educ")

education<-data.frame(educ_sum, label)


#barplot(education$educ_sum, main="Data Scientist Positions by Education", 
#        ylab="Education", xlab="Proportion of Job Posts", horiz=TRUE, names.arg=c("PhD", "MS", "BS"))

##Create box plot 
data$educ_cat2<-ifelse(data$educ_cat == "", "1. No Education" , data$educ_cat)
data$educ_cat2<-ifelse(data$educ_cat == "BS", "2. BS" , data$educ_cat2)
data$educ_cat2<-ifelse(data$educ_cat == "MS", "3. MS" , data$educ_cat2)
data$educ_cat2<-ifelse(data$educ_cat == "PhD", "4. PHD" , data$educ_cat2)
data$educ_cat2<-ifelse(data$educ_cat == "BS MS PhD", "5. BS MS PhD" , data$educ_cat2)

avg1<-round(mean(data$salary_average[data$educ_cat2=="1. No Education"]),1)
sd1<-round(sd(data$salary_average[data$educ_cat2=="1. No Education"]),1)
avg2<-round(mean(data$salary_average[data$educ_cat2=="2. BS"]),1)
sd2<-round(sd(data$salary_average[data$educ_cat2=="2. BS"]),1)
avg3<-round(mean(data$salary_average[data$educ_cat2=="3. MS"]),1)
sd3<-round(sd(data$salary_average[data$educ_cat2=="3. MS"]),1)
avg4<-round(mean(data$salary_average[data$educ_cat2=="4. PHD"]),1)
sd4<-round(sd(data$salary_average[data$educ_cat2=="4. PHD"]),1)


ht <- hux(
        Degree = c('No Education', 'Bachelors (BS)', 'Masters (MS)', 'PhD'),
        Jobs = c(" 99  (13.3%)", "175 (23.4%)", "255 (34.1%)", "218 (29.1%)"),
        Salaries = c(paste0(avg1, " (", sd1, ")"), paste0(avg2, " (", sd2, ")"), paste0(avg3, " (", sd3, ")"), paste0(avg4, " (", sd4, ")")),
        add_colnames = TRUE
      )%>%

      set_bold(1, everywhere, TRUE)       %>%
      set_bottom_border(1, everywhere, 1) %>%
      set_align(everywhere, 2, 'center')   %>%
      set_right_padding(10)               %>%
      set_left_padding(10)                %>%
      set_width(0.35)

ht


```

<br>
Overall, we were not able to get educational requrements for `r no_educ`% of job posts. Among job descriptions where education was specified, `r bs`%, `r masters`%, `r phd`%, mentioned a bachelors, masters, and PhD degrees respectively. Data science positions that mentionded a PhD had an average salary of $192,800, compared to $179,900 for masters, and $165,200 when only a bachelors degree was mentioned. For jobs where no information on education was ascertained, the average salary was similar to the overall average.

```{r, echo=FALSE}

boxplot(salary_average~educ_cat2,data=data, main="Differences in Estimated Salaries by Education", 
  	xlab="Estimated Salary", ylab="Estimated Salary (in thousands)")

```


```{r, include=FALSE}

educ_cleaned<-data[data$educ_cat2!="1. No Education",]

model2<-lm(educ_cleaned$salary_average~ educ_cleaned$educ_cat2)

summary(model2)

```

Using linear regression we confirmed differences in estimated salaries by educational requrements are statitically significant.  Compared to jobs where bachelors degree is the only educational requirement mentioned, jobs that mentioned masters degrees had an estimated salary 14.7 (SE: 2.7) thousand higher, jobs that mentioned a PhD had an estimated salary 27.7 (SE: 4.9) thousand higher. 


### Job Skills

Using the raw text from the job and employer decsipription I created binary variables looking for text relevant skills specific to data scientists, e.g., R, SQL, and Python. Overall, 91.5% of the 990 job posts had at least 1 of the 15 skills that were investigated.

Overall, the most common skills are Python, SQL, and R. Some less common skills include Oracle, Ruby , and Stata.

```{r, echo=FALSE}

SAS <-round((100*mean(data$sas==1)),1)
oracle <-round((100*mean(data$oracle==1)),1)
ruby <-round((100*mean(data$ruby==1)),1)
hadoop <-round((100*mean(data$hadoop==1)),1)
NLP <-round((100*mean(data$NLP==1)),1)
matlab <-round((100*mean(data$matlab==1)),1)
linux <-round((100*mean(data$linux==1)),1)
java <-round((100*mean(data$java==1)),1)
spss <-round((100*mean(data$spss==1)),1)
java <-round((100*mean(data$java==1)),1)
tableau <-round((100*mean(data$tableau==1)),1)
excel <-round((100*mean(data$excel==1)),1)
SQL <-round((100*mean(data$SQL==1)),1)
stata <-round((100*mean(data$stata==1)),1)
C <-round((100*mean(data$C==1)),1)
SAS <-round((100*mean(data$SAS==1)),1)
R <-round((100*mean(data$R==1)),1)
python <-round((100*mean(data$python==1)),1)
ml <-round((100*mean(data$ml==1)),1)
opt <-round((100*mean(data$opt==1)),1)
bd <-round((100*mean(data$bd==1)),1)
research <-round((100*mean(data$research==1)),1)
stats <-round((100*mean(data$stats==1)),1)
dm <-round((100*mean(data$dm==1)),1)
risk <-round((100*mean(data$risk==1)),1)
fs <-round((100*mean(data$fs==1)),1)
program <-round((100*mean(data$program==1)),1)
pharma <-round((100*mean(data$pharma==1)),1)
sd <-round((100*mean(data$sd==1)),1)
UX <-round((100*mean(data$UX==1)),1)

percent<-c(SAS, oracle, ruby, hadoop, NLP, matlab, linux, java, spss, tableau, excel, SQL, stata, C, R, python, ml, opt, bd, research, stats, dm, sd, UX)

label<-c("SAS", "oracle", "ruby", "hadoop", "NLP", "matlab", "linux", "java", "spss", "tableau", "excel", "SQL", "stata", "C", "R", "python", "machine learning", "optimization", "big data", "research", "statistics", "data management", "software development", "UX")


skills<-data.frame(percent, label)


skills<-skills[order(percent),]


p<-ggplot(data=skills, aes(x=reorder(label, -percent), y=percent)) +
  geom_bar(stat="identity", fill="steelblue")+
  theme_minimal()+theme(axis.text.x = element_text(angle = 60, hjust = 1))+
 ggtitle("Data Scientist Skills")+labs(x="Skills", y="Percent of Jobs")
p

```

###Job Skills and Estimated Salary

Here I am going to include a figure looking at the associations of the individual skills and estimated salary (adjusting for differences in education). e.g., i will include the point estimates and 95% CI for the betas for the individual skills from each model.

The results make sense, Machine Learning had the largest estimate, and tableau and excel the lowest.




```{r, include=FALSE}

m_stats<-lm(educ_cleaned$salary_average~educ_cleaned$stats+educ_cleaned$educ_cat)
m_python<-lm(educ_cleaned$salary_average~educ_cleaned$python+educ_cleaned$educ_cat)
m_SQL<-lm(educ_cleaned$salary_average~educ_cleaned$SQL+educ_cleaned$educ_cat)
m_research<-lm(educ_cleaned$salary_average~educ_cleaned$research+educ_cleaned$educ_cat)
m_ml<-lm(educ_cleaned$salary_average~educ_cleaned$ml+educ_cleaned$educ_cat)
m_r<-lm(educ_cleaned$salary_average~educ_cleaned$R+educ_cleaned$educ_cat)
m_bd<-lm(educ_cleaned$salary_average~educ_cleaned$bd+educ_cleaned$educ_cat)
m_hadoop<-lm(educ_cleaned$salary_average~educ_cleaned$hadoop+educ_cleaned$educ_cat)
m_java<-lm(educ_cleaned$salary_average~educ_cleaned$java+educ_cleaned$educ_cat)
m_C<-lm(educ_cleaned$salary_average~educ_cleaned$C+educ_cleaned$educ_cat)
m_SAS<-lm(educ_cleaned$salary_average~educ_cleaned$SAS+educ_cleaned$educ_cat)
m_Opt<-lm(educ_cleaned$salary_average~educ_cleaned$opt+educ_cleaned$educ_cat)
m_excel<-lm(educ_cleaned$salary_average~educ_cleaned$excel+educ_cleaned$educ_cat)
m_tableau<-lm(educ_cleaned$salary_average~educ_cleaned$tableau+educ_cleaned$educ_cat)
m_dm<-lm(educ_cleaned$salary_average~educ_cleaned$dm+educ_cleaned$educ_cat)
m_matlab<-lm(educ_cleaned$salary_average~educ_cleaned$matlab+educ_cleaned$educ_cat)
m_sd<-lm(educ_cleaned$salary_average~educ_cleaned$sd+educ_cleaned$educ_cat)
m_linux<-lm(educ_cleaned$salary_average~educ_cleaned$linux+educ_cleaned$educ_cat)
m_spss<-lm(educ_cleaned$salary_average~educ_cleaned$spss+educ_cleaned$educ_cat)
m_oracle<-lm(educ_cleaned$salary_average~educ_cleaned$oracle+educ_cleaned$educ_cat)
m_nlp<-lm(educ_cleaned$salary_average~educ_cleaned$NLP+educ_cleaned$educ_cat)
m_ruby<-lm(educ_cleaned$salary_average~educ_cleaned$ruby+educ_cleaned$educ_cat)
m_UX<-lm(educ_cleaned$salary_average~educ_cleaned$UX+educ_cleaned$educ_cat)
m_stata<-lm(educ_cleaned$salary_average~educ_cleaned$UX+educ_cleaned$educ_cat)

#models<-c('m_stats', 'm_python', 'm_SQL', 'm_research', 'm_ml', 'm_r', 'm_bd', 'm_hadoop', 'm_java', 'm_C', 'm_SAS', 'm_Opt', 'm_excel', #'m_tableau', 'm_dm', 'm_matlab', 'm_sd', 'm_linux', 'm_spss', 'm_oracle', 'm_nlp', 'm_ruby', 'm_UX', 'm_stata')
#betas<-c('educ_cleaned$statsTRUE', 'educ_cleaned$pythonTRUE', 'educ_cleaned$SQLTRUE', 'educ_research$TRUE', 'educ_cleaned$mlTRUE', #'educ_cleaned$RTRUE', 'educ_cleaned$BDTRUE', 'educ_cleaned$hadoopTRUE', 'educ_cleaned$javaTRUE', 'educ_cleaned$CTRUE', #'educ_cleaned$SASTRUE', 'educ_cleaned$optTRUE', 'educ_cleaned$excelTRUE', 'educ_cleaned$tableauTRUE', 'educ_cleaned$dmTRUE', #'educ_cleaned$matlabTRUE', 'educ_cleaned$sdTRUE', 'educ_cleaned$linuxTRUE', 'educ_cleaned$spssTRUE', 'educ_cleaned$oracleTRUE', #'educ_cleaned$NLPTRUE', 'educ_cleaned$rubyTRUE', 'educ_cleaned$UXTRUE', 'educ_cleaned$stataTRUE')
#m_stats


#huxreg(m_ruby, m_UX, m_stata)
#m_stats$coefficients[1]
#m_stats$coefficients[1]

#for (i in 1:24)
#  {
#    lcl<-confint(models[i], betas[i], level=0.95)[1]
#}

#summary(paste0(models[1]))
```

###Conclusion

Here I will tie everything together